{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdiZQ7ZDpdEx"
      },
      "source": [
        "# Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xub4YEnpdE8"
      },
      "source": [
        "## Understanding the environment\n",
        "We are using a library that has everything you need to create a simulation\n",
        "https://www.gymlibrary.dev/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB-M0ypepdE_",
        "outputId": "05a7af9a-b08f-4f06-a74a-e34bc9add025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gym) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gym) (0.0.8)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "zsh:1: no matches found: gym[classic_control]\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gym\n",
        "%pip install gym[classic_control]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJTXHkRcpdFC"
      },
      "source": [
        "We create a simulation\n",
        "\n",
        "<img src=\"cart_pole.gif\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "gKukB0dPpdFD",
        "outputId": "06796cda-a764-42ef-adac-aec16cf66426"
      },
      "outputs": [
        {
          "ename": "DependencyNotInstalled",
          "evalue": "pygame is not installed, run `pip install gym[classic_control]`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:219\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpygame\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpygame\u001b[39;00m \u001b[39mimport\u001b[39;00m gfxdraw\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygame'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m\"\u001b[39m, render_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mseed(\u001b[39m42\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m observation, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset(seed\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m      9\u001b[0m     observation, reward, terminated, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample())\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/wrappers/time_limit.py:68\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    The reset environment\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:42\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/wrappers/env_checker.py:45\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_reset \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m env_reset_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:192\u001b[0m, in \u001b[0;36menv_reset_passive_checker\u001b[0;34m(env, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    188\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFuture gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[39m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    195\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    196\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m     )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:206\u001b[0m, in \u001b[0;36mCartPoleEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_beyond_terminated \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), {}\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:222\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpygame\u001b[39;00m \u001b[39mimport\u001b[39;00m gfxdraw\n\u001b[1;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[39mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[1;32m    223\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpygame is not installed, run `pip install gym[classic_control]`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     pygame\u001b[39m.\u001b[39minit()\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gym[classic_control]`"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "env.action_space.seed(42)\n",
        "\n",
        "observation, info = env.reset(seed=42)\n",
        "\n",
        "for _ in range(1000):\n",
        "    observation, reward, terminated, _, info = env.step(env.action_space.sample())\n",
        "\n",
        "    if terminated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI0VlWm8pdFE"
      },
      "source": [
        "### How does it work?\n",
        "Let's comprehend the actions and states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYpFkUAbpdFF",
        "outputId": "bf38af2a-5064-46b3-f645-efacf81c7a36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_space\n",
        "# 0 - Left\n",
        "# 1 - Right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3Hr44QgpdFG",
        "outputId": "b6c8103a-d364-463f-e772-7cdda7c386a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.00384444, -0.02675204, -0.01892   , -0.01974045], dtype=float32)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "obs #Vertical Position, Velocity, Angle, Angular Velocity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm5hX9X1pdFH"
      },
      "source": [
        "### Progression\n",
        "\n",
        "Each step we do in the simulation is acting according to the action we pass to the simulation. It changes the actual state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eidtP_fPpdFI",
        "outputId": "7ac3017d-1816-4f3c-e69a-a02b6e805228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.0311069   0.01699071 -0.02974914 -0.02360288]\n",
            "(array([-0.03076709, -0.17769226, -0.03022119,  0.2595474 ], dtype=float32), 1.0, False, {})\n"
          ]
        }
      ],
      "source": [
        "print( env.reset() )\n",
        "print( env.step(0) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0SRRvdBpdFJ"
      },
      "source": [
        "Adtional variables that returns:\n",
        "\n",
        "*   Reward - Positive if doing the right action, negative if not\n",
        "*   Done - If we end the simulation\n",
        "*   Truncate - If the simulation overpasses a limit steps \n",
        "*   Aditional Information - Extra info the environment can use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whp5gFjGpdFJ"
      },
      "source": [
        "### Basic Simulation\n",
        "\n",
        "We can try to keep the cartpole facing up.\n",
        "\n",
        "1. If the angle is less than 0, it should move to the left\n",
        "2. If the angle is more than 0, it should move to the rifht"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPE-APWKpdFK"
      },
      "outputs": [],
      "source": [
        "def relgas_basicas(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1 # 0 left, 1 right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "udDDEFzQpdFK",
        "outputId": "1dc87777-de3b-4b42-9f12-e6f09502aa4c"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-605394c0b0eb>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrecompenzas_episodio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 1000 steps max, we don't want to run forever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelgas_basicas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \"\"\"\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;34m\"\"\"Resets the environment with kwargs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     def render(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_reset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_reset_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_reset_passive_checker\u001b[0;34m(env, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# Checks the result of env.reset with kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"return_info\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         assert isinstance(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, return_info, options)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_beyond_terminated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/renderer.py\u001b[0m in \u001b[0;36mrender_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_render\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mrender_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_returns_render\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m                 self.screen = pygame.display.set_mode(\n\u001b[1;32m    234\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: No available video device"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "\n",
        "totals = []\n",
        "\n",
        "for episode in range(20):\n",
        "    recompenzas_episodio = 0\n",
        "    obs = env.reset()[0]\n",
        "    for step in range(1000): # 1000 steps max, we don't want to run forever\n",
        "        action = relgas_basicas(obs)\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        recompenzas_episodio += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(recompenzas_episodio)\n",
        "    \n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5tBnbp6pdFL"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH0LRODgpdFM",
        "outputId": "315c5b6c-808a-4884-e617-18f0f1ec028a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(45.95, 9.40996811896831, 34.0, 64.0)"
            ]
          },
          "execution_count": 560,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoekyDabpdFM"
      },
      "source": [
        "## Using Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqO_ka8lpdFN",
        "outputId": "5ff29643-6fff-4c7c-a3e4-353fa9e56acf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([-0.0232647 , -0.00226463, -0.04544467, -0.03048344], dtype=float32),\n",
              " {})"
            ]
          },
          "execution_count": 613,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfPn6KcZpdFN"
      },
      "source": [
        "### Preparing Q-Table\n",
        "<img src=\"q-table.png\">\n",
        "\n",
        "We have numerical and continuous values.\n",
        "We can discretize them to reduce the shape of the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X2jLJBJpdFO"
      },
      "outputs": [],
      "source": [
        "bin = 50 # grupos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNDp8nr5pdFO"
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros(shape=(bin,bin,bin,bin,2)) # 4 dimensiones de estados y las acciones.\n",
        "q_table.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KCtTvDopdFO"
      },
      "source": [
        "### Discretizing\n",
        "\n",
        "Let's do this in an intuitive way\n",
        "\n",
        "<img src=\"https://datascientest.com/es/wp-content/uploads/sites/7/2020/12/illu_normali_blog-49-1024x562.png\" width=300>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8PdgmfPpdFP",
        "outputId": "557fda4a-b510-4faf-cd58-10b41b0da7d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
          ]
        }
      ],
      "source": [
        "valores = np.arange(10,100)\n",
        "grupos = 5\n",
        "\n",
        "separación = [ round( ( valor - valores.min() )  / ( valores.max() - valores.min() ) * grupos )  for valor in valores]\n",
        "print( separación )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V6Wp3pkpdFP"
      },
      "outputs": [],
      "source": [
        "def discretize(state):\n",
        "    h=env.observation_space.high \n",
        "    l=env.observation_space.low\n",
        "    \n",
        "    if type(state) is tuple:\n",
        "        state = state[0]\n",
        "    \n",
        "    aux = ( (state - l) / (h-l) ) * bin\n",
        "    return tuple( aux.astype('int32') )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqS-MZJ7pdFQ",
        "outputId": "fc0d6961-c8ec-4207-9764-14de7f58ff89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\c28_0\\AppData\\Local\\Temp\\ipykernel_11656\\191351855.py:8: RuntimeWarning: overflow encountered in subtract\n",
            "  aux = ( (state - l) / (h-l) ) * bin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(25, 0, 23, 0)"
            ]
          },
          "execution_count": 618,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "discretize( env.reset() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLKtFTNJpdFQ"
      },
      "source": [
        "El obtener una tupla con los valores del indice de la tabla, nos permite usarlos como indices para el arreglo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB-UyHZipdFQ",
        "outputId": "b23a0a99-2c8e-4690-a21a-bd49509d7d16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\c28_0\\AppData\\Local\\Temp\\ipykernel_11656\\191351855.py:8: RuntimeWarning: overflow encountered in subtract\n",
            "  aux = ( (state - l) / (h-l) ) * bin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0., 0.])"
            ]
          },
          "execution_count": 619,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_table[ discretize( env.reset() ) ] # nos traerá las posibles acciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhbOGNyApdFR"
      },
      "source": [
        "### Trainning the model\n",
        "\n",
        "<img src=\"q-learning.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h54W6MLvpdFR",
        "outputId": "2e3c2dea-36ac-48c4-d48a-e686f6f12b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 49900\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#inicialización de simulación\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Hyperparametros\n",
        "aprendizaje = 0.1 # Taza aprendizaje\n",
        "descuento = 0.9 # Taza descuento\n",
        "\n",
        "epsilon = 0.1 # Umbral de aleatoriedad\n",
        "\n",
        "for i in range(1, 50000):\n",
        "    state = env.reset() # Reseteamos los valores\n",
        "\n",
        "    final = False\n",
        "    \n",
        "    while not final:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explora espacio de acciones\n",
        "        else:\n",
        "            action = np.argmax(q_table[discretize(state)]) # Usa valores aprendidos\n",
        "        \n",
        "        valor_siguiente, recompenza, final, _, info = env.step(action)\n",
        "        \n",
        "        valor_actual = q_table[discretize(state)][action]\n",
        "        valor_futuro = np.max(q_table[discretize(valor_siguiente)])\n",
        "        \n",
        "        nuevo_valor = valor_actual + ( aprendizaje * ( recompenza + ( descuento * valor_futuro ) - valor_actual ) )\n",
        "        q_table[discretize(state)][action] = nuevo_valor\n",
        "\n",
        "        state = valor_futuro\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOFEIroVpdFS"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce1kI-IapdFS",
        "outputId": "76e943e1-45c5-4354-b46e-173f9421b951"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\c28_0\\AppData\\Local\\Temp\\ipykernel_11656\\191351855.py:8: RuntimeWarning: overflow encountered in subtract\n",
            "  aux = ( (state - l) / (h-l) ) * bin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados despues de 1000 episodios:\n",
            "30.974 13.258254937962235 12 94\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()\n",
        "\n",
        "total_rewards = []\n",
        "episodes = 1000\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    rewards = 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[discretize(state)])\n",
        "        state, reward, done, _, info = env.step(action)\n",
        "\n",
        "        if reward == 1:\n",
        "            rewards += 1\n",
        "\n",
        "    total_rewards.append( rewards )\n",
        "env.close()\n",
        "\n",
        "print(f\"Resultados despues de {episodes} episodios:\")\n",
        "print( np.mean(total_rewards), np.std(total_rewards), np.min(total_rewards), np.max(total_rewards) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Ak5a3jPmwn"
      },
      "source": [
        "## Creating our own Simulations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYks1cZjpdFX"
      },
      "outputs": [],
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT-ZPR8l2oV1"
      },
      "source": [
        "## Example of a simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnp_VBHzQc_h"
      },
      "outputs": [],
      "source": [
        "class ReguladorSonido(Env):\n",
        "  def __init__(self):\n",
        "    # Accciones: \n",
        "    #Subir bocina, bajar bocina, subir youtube, bajar youtube, mantener volumen\n",
        "    self.action_space = Discrete(5)\n",
        "    # Valores posibles\n",
        "    # Sonido Bocina  [ 1 - 100 ]\n",
        "    # Sonido Youtube [ 1 - 100 ]\n",
        "    self.observation_space = Box(low=np.array([0,0]), high=np.array([100,100]))\n",
        "    # Set inicio\n",
        "    self.state = tuple( [ random.randint(0,100), random.randint(0,100) ] )\n",
        "    # Set Límite ( Canción de 2 minutos)\n",
        "    self.duracion_cancion = 120\n",
        "    self.epoch = 0 # segundo actual\n",
        "\n",
        "  def step(self, action):\n",
        "    done = False\n",
        "\n",
        "    # Aplicar accion\n",
        "    # 0 - Bajar Volumen Bocina\n",
        "    # 1 - Subir Volumen Bocina\n",
        "    # 2 - Bajar Volumen Youtube\n",
        "    # 3 - Subir Volumen Youtube\n",
        "    # 4 - Mantener Volumen\n",
        "    if action == 0:\n",
        "      self.state = tuple( [ max( self.state[0] -1, 0 ), self.state[1] ] )\n",
        "    if action == 1:\n",
        "      self.state = tuple( [ min(self.state[0] +1, 99 ), self.state[1] ] )\n",
        "    if action == 2:\n",
        "      self.state = tuple( [ self.state[0], max( self.state[1] -1, 0 ) ] )\n",
        "    if action == 3:\n",
        "      self.state = tuple( [ self.state[0], min( self.state[1] +1, 99) ] )\n",
        "    if action == 4:\n",
        "      self.state = self.state \n",
        "\n",
        "    # Cada segundo que pasa, la canción va acabando\n",
        "    self.epoch = self.epoch + 1\n",
        "\n",
        "    # Estimando el premio\n",
        "    volumen_total = np.array( self.state ).mean()\n",
        "    if volumen_total > 60 and volumen_total < 65:\n",
        "      reward = 10\n",
        "      done = True\n",
        "    else:\n",
        "      reward = -1\n",
        "\n",
        "\n",
        "    # Verificar si acabó la canción o se llego al valor esperado\n",
        "    if self.duracion_cancion == self.epoch and not done: \n",
        "      done = True\n",
        "\n",
        "    # Set placeholder for info\n",
        "    info = {}\n",
        "\n",
        "    # Return step information\n",
        "    return self.state, reward, done, self.epoch, info\n",
        "\n",
        "  def render(self):\n",
        "    # Implement viz\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    # Reseteo de volumen\n",
        "    self.state = tuple( [ random.randint(30,90), random.randint(30,90) ] )\n",
        "    # Reseteo de canción\n",
        "    self.duracion_cancion = 120\n",
        "    # Reseteo de epoch\n",
        "    self.epoch = 0\n",
        "\n",
        "    return self.state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXwGLejkeTVG"
      },
      "source": [
        "### Creating the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fD33wD4dYHs",
        "outputId": "cda0e712-a2d1-4ee4-d6d4-cbaec4d35d9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(58, 66)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = ReguladorSonido()\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZhpBryIXHaD"
      },
      "source": [
        "## Q - Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bceE2IX1XKRZ",
        "outputId": "7d1b0d87-8bce-45ff-9975-95cba4a759b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Box(0.0, 100.0, (2,), float32)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klHh9oMQYc0L",
        "outputId": "b9821d51-4ed9-499c-c8d5-fdd37a27282c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDjZGe60ekDW"
      },
      "outputs": [],
      "source": [
        "acciones = env.action_space.n\n",
        "observaciones = env.observation_space.high[0].astype('int32')\n",
        "q_table = np.zeros( [observaciones,observaciones, acciones] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zg1ONjFhDn-",
        "outputId": "ba31d3f5-7a99-4590-bb6e-61e3ff75b4fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 100, 5)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_table.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLtutcEdhtBi"
      },
      "source": [
        "### Trainning the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "37XKl66hhbbk",
        "outputId": "807777ff-9670-4fb2-cf5a-2c9125ba3830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 81600\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-448f9286049e>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvalor_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestado\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mvalor_futuro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msiguiente_estado\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnuevo_valor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalor_actual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0maprendizaje\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mrecompenza\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mdescuento\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalor_futuro\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalor_actual\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2789\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2790\u001b[0m     \"\"\"\n\u001b[0;32m-> 2791\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2792\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "aprendizaje = 0.1\n",
        "descuento = 0.9\n",
        "epsilon = 0.1 # variabe control\n",
        "\n",
        "for i in range(1, 200001):\n",
        "    estado = env.reset()\n",
        "    done = False # bandera de finalización\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[estado]) # Exploit learned values\n",
        "\n",
        "        siguiente_estado, recompenza, done, _, info = env.step(action)\n",
        "        \n",
        "        valor_actual = q_table[estado][action]\n",
        "        valor_futuro = np.max(q_table[siguiente_estado])\n",
        "        \n",
        "        nuevo_valor = valor_actual + ( aprendizaje * ( recompenza + ( descuento * valor_futuro ) - valor_actual ) )\n",
        "        q_table[estado][action] = nuevo_valor\n",
        "\n",
        "        estado = siguiente_estado\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGRLqbRno86-"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlVRixhDoysI",
        "outputId": "cdacbe87-b4ad-4db0-a14b-e74ed2ee79c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 30.51\n",
            "Average rewards per episode: -21.6\n"
          ]
        }
      ],
      "source": [
        "total_epochs, total_reward = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "env = ReguladorSonido()\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    env.render()\n",
        "    epochs, reward = 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "      action = np.argmax(q_table[state])\n",
        "      state, reward, done, _, info = env.step(action)\n",
        "      epochs += 1\n",
        "\n",
        "      total_reward += reward\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average rewards per episode: {total_reward / episodes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyH8n0cPaRMd"
      },
      "outputs": [],
      "source": [
        "q_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnfQORPbpdFT"
      },
      "source": [
        "## Q-Learning Ejercicio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOQJe42ipdFT",
        "outputId": "552ec7cf-3447-4973-fc3a-e23755b5a5f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[atari] in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (1.23.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (2.2.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (0.0.8)\n",
            "Collecting ale-py~=0.8.0\n",
            "  Downloading ale_py-0.8.0-cp310-cp310-win_amd64.whl (950 kB)\n",
            "     -------------------------------------- 950.8/950.8 kB 1.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ale-py~=0.8.0->gym[atari]) (4.4.0)\n",
            "Collecting importlib-resources\n",
            "  Downloading importlib_resources-5.10.2-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: importlib-resources, ale-py\n",
            "Successfully installed ale-py-0.8.0 importlib-resources-5.10.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script ale-import-roms.exe is installed in 'C:\\Users\\c28_0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[atari]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEuoU1uApdFT"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"Taxi-v3\", render_mode='human')\n",
        "\n",
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "\n",
        "print(\"Acciones {}\".format(env.action_space))\n",
        "print(\"Estados {}\".format(env.observation_space))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiOT3D1HpdFU"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpr53CSUpdFU"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlQKCO5spdFV",
        "outputId": "1baaad38-b08f-4eef-e6a4-23570a2241ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discrete(500)"
            ]
          },
          "execution_count": 636,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp1e0EmpZoqy"
      },
      "source": [
        "Create the Q table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4wdhC3xZqCP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVWGQshTpdFV",
        "outputId": "40586b34-2166-421c-c87b-5744c17f3f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "# Hyperparameters\n",
        "aprendizaje = \n",
        "descuento = \n",
        "epsilon = \n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        siguiente_estado, recompenza, done, _, info = env.step(action) \n",
        "        \n",
        "        valor_actual = q_table[state, action]\n",
        "        valor_futuro = np.max(q_table[siguiente_estado])\n",
        "        \n",
        "        nuevo_valor = valor_actual + ( aprendizaje * ( recompenza + ( descuento * valor_futuro ) - valor_actual ) )\n",
        "        q_table[state, action] = nuevo_valor\n",
        "\n",
        "        state = siguiente_estado\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUswwLFypdFW",
        "outputId": "6470bc2b-9995-498f-ade5-c7fa8715a699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.84\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ],
      "source": [
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "\n",
        "for _ in range(episodes):\n",
        "    env = gym.make(\"Taxi-v3\", render_mode='human')\n",
        "    state = env.reset()[0]\n",
        "    env.render()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, _, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "    env.close()\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwN4wJo1pdFX"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9c0c29ab910d078a8b86acec451c12e2964e85d2ce873541b902fdf373b0ea19"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
