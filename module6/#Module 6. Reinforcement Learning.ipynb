{"cells":[{"cell_type":"markdown","metadata":{"id":"pdiZQ7ZDpdEx"},"source":["# Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"4Xub4YEnpdE8"},"source":["## Understanding the environment\n","We are using a library that has everything you need to create a simulation\n","https://www.gymlibrary.dev/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3226,"status":"ok","timestamp":1683507189946,"user":{"displayName":"Christopher Orea","userId":"07171859992703395178"},"user_tz":-540},"id":"cB-M0ypepdE_","outputId":"8733ed71-e668-4795-d32e-6ef8b831bf4e"},"outputs":[{"name":"stderr","output_type":"stream","text":["170.67s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: gym in /Users/apple/Library/Python/3.9/lib/python/site-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in /Users/apple/Library/Python/3.9/lib/python/site-packages (from gym) (1.24.3)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /Users/apple/Library/Python/3.9/lib/python/site-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /Users/apple/Library/Python/3.9/lib/python/site-packages (from gym) (0.0.8)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/apple/Library/Python/3.9/lib/python/site-packages (from gym) (6.6.0)\n","Requirement already satisfied: zipp>=0.5 in /Users/apple/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["176.61s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"]},{"name":"stdout","output_type":"stream","text":["zsh:1: no matches found: gym[classic_control]\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install gym\n","%pip install gym[classic_control]"]},{"cell_type":"markdown","metadata":{"id":"EJTXHkRcpdFC"},"source":["We create a simulation\n","\n","<img src=\"cart_pole.gif\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKukB0dPpdFD"},"outputs":[],"source":["import gym\n","\n","env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n","env.action_space.seed(42)\n","\n","observation, info = env.reset(seed=42)\n","\n","for _ in range(1000):\n","    observation, reward, terminated, _, info = env.step(env.action_space.sample())\n","\n","    if terminated:\n","        observation, info = env.reset()\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"XI0VlWm8pdFE"},"source":["### How does it work?\n","Let's comprehend the actions and states"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eYpFkUAbpdFF","outputId":"64c4eaea-d0b9-4e29-f1a9-65adbc69f978"},"outputs":[{"data":{"text/plain":["Discrete(2)"]},"execution_count":554,"metadata":{},"output_type":"execute_result"}],"source":["env.action_space\n","# 0 - Left\n","# 1 - Right"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3Hr44QgpdFG","outputId":"7c528eab-a48e-4b6e-efec-fe29383f827f"},"outputs":[{"data":{"text/plain":["(array([-0.03923135, -0.01530885, -0.04146622, -0.02042414], dtype=float32),\n"," {})"]},"execution_count":555,"metadata":{},"output_type":"execute_result"}],"source":["env = gym.make(\"CartPole-v1\")\n","obs = env.reset()\n","obs #Vertical Position, Velocity, Angle, Angular Velocity"]},{"cell_type":"markdown","metadata":{"id":"lm5hX9X1pdFH"},"source":["### Progression\n","\n","Each step we do in the simulation is acting according to the action we pass to the simulation. It changes the actual state."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eidtP_fPpdFI","outputId":"5a1fdd1c-5e1d-45f9-e67c-ad86eb27607c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(array([ 0.00804425, -0.03628454, -0.04908066, -0.02724105], dtype=float32), {})\n","(array([ 0.00731855, -0.23066953, -0.04962548,  0.24956138], dtype=float32), 1.0, False, False, {})\n"]}],"source":["print( env.reset() )\n","print( env.step(0) )"]},{"cell_type":"markdown","metadata":{"id":"c0SRRvdBpdFJ"},"source":["Adtional variables that returns:\n","\n","*   Reward - Positive if doing the right action, negative if not\n","*   Done - If we end the simulation\n","*   Truncate - If the simulation overpasses a limit steps \n","*   Aditional Information - Extra info the environment can use."]},{"cell_type":"markdown","metadata":{"id":"Whp5gFjGpdFJ"},"source":["### Basic Simulation\n","\n","We can try to keep the cartpole facing up.\n","\n","1. If the angle is less than 0, it should move to the left\n","2. If the angle is more than 0, it should move to the rifht"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPE-APWKpdFK"},"outputs":[],"source":["def relgas_basicas(obs):\n","    angle = obs[2]\n","    return 0 if angle < 0 else 1 # 0 left, 1 right"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udDDEFzQpdFK"},"outputs":[],"source":["env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n","\n","totals = []\n","\n","for episode in range(20):\n","    recompenzas_episodio = 0\n","    obs = env.reset()[0]\n","    for step in range(1000): # 1000 steps max, we don't want to run forever\n","        action = relgas_basicas(obs)\n","        obs, reward, done, _, info = env.step(action)\n","        recompenzas_episodio += reward\n","        if done:\n","            break\n","    totals.append(recompenzas_episodio)\n","    \n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"K5tBnbp6pdFL"},"source":["### Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qH0LRODgpdFM","outputId":"315c5b6c-808a-4884-e617-18f0f1ec028a"},"outputs":[{"data":{"text/plain":["(45.95, 9.40996811896831, 34.0, 64.0)"]},"execution_count":560,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"]},{"cell_type":"markdown","metadata":{"id":"FoekyDabpdFM"},"source":["## Using Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqO_ka8lpdFN","outputId":"5ff29643-6fff-4c7c-a3e4-353fa9e56acf"},"outputs":[{"data":{"text/plain":["(array([-0.0232647 , -0.00226463, -0.04544467, -0.03048344], dtype=float32),\n"," {})"]},"execution_count":613,"metadata":{},"output_type":"execute_result"}],"source":["env = gym.make(\"CartPole-v1\")\n","env.reset()"]},{"cell_type":"markdown","metadata":{"id":"mfPn6KcZpdFN"},"source":["### Preparing Q-Table\n","<img src=\"q-table.png\">\n","\n","We have numerical and continuous values.\n","We can discretize them to reduce the shape of the table."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3X2jLJBJpdFO"},"outputs":[],"source":["bin = 50 # grupos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YNDp8nr5pdFO"},"outputs":[],"source":["q_table = np.zeros(shape=(bin,bin,bin,bin,2)) # 4 dimensiones de estados y las acciones.\n","q_table.shape"]},{"cell_type":"markdown","metadata":{"id":"8KCtTvDopdFO"},"source":["### Discretizing\n","\n","Let's do this in an intuitive way\n","\n","<img src=\"https://datascientest.com/es/wp-content/uploads/sites/7/2020/12/illu_normali_blog-49-1024x562.png\" width=300>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8PdgmfPpdFP","outputId":"557fda4a-b510-4faf-cd58-10b41b0da7d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"]}],"source":["valores = np.arange(10,100)\n","grupos = 5\n","\n","separación = [ round( ( valor - valores.min() )  / ( valores.max() - valores.min() ) * grupos )  for valor in valores]\n","print( separación )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2V6Wp3pkpdFP"},"outputs":[],"source":["def discretize(state):\n","    h=env.observation_space.high \n","    l=env.observation_space.low\n","    \n","    if type(state) is tuple:\n","        state = state[0]\n","    \n","    aux = ( (state - l) / (h-l) ) * bin\n","    return tuple( aux.astype('int32') )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqS-MZJ7pdFQ","outputId":"fc0d6961-c8ec-4207-9764-14de7f58ff89"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\c28_0\\AppData\\Local\\Temp\\ipykernel_11656\\191351855.py:8: RuntimeWarning: overflow encountered in subtract\n","  aux = ( (state - l) / (h-l) ) * bin\n"]},{"data":{"text/plain":["(25, 0, 23, 0)"]},"execution_count":618,"metadata":{},"output_type":"execute_result"}],"source":["discretize( env.reset() )"]},{"cell_type":"markdown","metadata":{"id":"VLKtFTNJpdFQ"},"source":["El obtener una tupla con los valores del indice de la tabla, nos permite usarlos como indices para el arreglo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gB-UyHZipdFQ","outputId":"b23a0a99-2c8e-4690-a21a-bd49509d7d16"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\c28_0\\AppData\\Local\\Temp\\ipykernel_11656\\191351855.py:8: RuntimeWarning: overflow encountered in subtract\n","  aux = ( (state - l) / (h-l) ) * bin\n"]},{"data":{"text/plain":["array([0., 0.])"]},"execution_count":619,"metadata":{},"output_type":"execute_result"}],"source":["q_table[ discretize( env.reset() ) ] # nos traerá las posibles acciones"]},{"cell_type":"markdown","metadata":{"id":"mhbOGNyApdFR"},"source":["### Trainning the model\n","\n","<img src=\"q-learning.png\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h54W6MLvpdFR","outputId":"2e3c2dea-36ac-48c4-d48a-e686f6f12b5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode: 49900\n","Training finished.\n","\n"]}],"source":["import random\n","from IPython.display import clear_output\n","\n","#inicialización de simulación\n","env = gym.make(\"CartPole-v1\")\n","\n","# Hyperparametros\n","aprendizaje = 0.1 # Taza aprendizaje\n","descuento = 0.9 # Taza descuento\n","\n","epsilon = 0.1 # Umbral de aleatoriedad\n","\n","for i in range(1, 50000):\n","    state = env.reset() # Reseteamos los valores\n","\n","    final = False\n","    \n","    while not final:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explora espacio de acciones\n","        else:\n","            action = np.argmax(q_table[discretize(state)]) # Usa valores aprendidos\n","        \n","        valor_siguiente, recompenza, final, _, info = env.step(action)\n","        \n","        valor_actual = q_table[discretize(state)][action]\n","        valor_futuro = np.max(q_table[discretize(valor_siguiente)])\n","        \n","        nuevo_valor = valor_actual + ( aprendizaje * ( recompenza + ( descuento * valor_futuro ) - valor_actual ) )\n","        q_table[discretize(state)][action] = nuevo_valor\n","\n","        state = valor_futuro\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"markdown","metadata":{"id":"pOFEIroVpdFS"},"source":["### Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce1kI-IapdFS","outputId":"76e943e1-45c5-4354-b46e-173f9421b951"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\c28_0\\AppData\\Local\\Temp\\ipykernel_11656\\191351855.py:8: RuntimeWarning: overflow encountered in subtract\n","  aux = ( (state - l) / (h-l) ) * bin\n"]},{"name":"stdout","output_type":"stream","text":["Resultados despues de 1000 episodios:\n","30.974 13.258254937962235 12 94\n"]}],"source":["env = gym.make(\"CartPole-v1\")\n","env.reset()\n","\n","total_rewards = []\n","episodes = 1000\n","\n","for _ in range(episodes):\n","    state = env.reset()\n","    rewards = 0\n","    done = False\n","    \n","    while not done:\n","        action = np.argmax(q_table[discretize(state)])\n","        state, reward, done, _, info = env.step(action)\n","\n","        if reward == 1:\n","            rewards += 1\n","\n","    total_rewards.append( rewards )\n","env.close()\n","\n","print(f\"Resultados despues de {episodes} episodios:\")\n","print( np.mean(total_rewards), np.std(total_rewards), np.min(total_rewards), np.max(total_rewards) )"]},{"cell_type":"markdown","metadata":{"id":"e5Ak5a3jPmwn"},"source":["## Creating our own Simulations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYks1cZjpdFX"},"outputs":[],"source":["from gym import Env\n","from gym.spaces import Discrete, Box\n","import numpy as np\n","import random"]},{"cell_type":"markdown","metadata":{"id":"VT-ZPR8l2oV1"},"source":["## Example of a simulation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vnp_VBHzQc_h"},"outputs":[],"source":["class ReguladorSonido(Env):\n","  def __init__(self):\n","    # Accciones: \n","    #Subir bocina, bajar bocina, subir youtube, bajar youtube, mantener volumen\n","    self.action_space = Discrete(5)\n","    # Valores posibles\n","    # Sonido Bocina  [ 1 - 100 ]\n","    # Sonido Youtube [ 1 - 100 ]\n","    self.observation_space = Box(low=np.array([0,0]), high=np.array([100,100]))\n","    # Set inicio\n","    self.state = tuple( [ random.randint(0,100), random.randint(0,100) ] )\n","    # Set Límite ( Canción de 2 minutos)\n","    self.duracion_cancion = 120\n","    self.epoch = 0 # segundo actual\n","\n","  def step(self, action):\n","    done = False\n","\n","    # Aplicar accion\n","    # 0 - Bajar Volumen Bocina\n","    # 1 - Subir Volumen Bocina\n","    # 2 - Bajar Volumen Youtube\n","    # 3 - Subir Volumen Youtube\n","    # 4 - Mantener Volumen\n","    if action == 0:\n","      self.state = tuple( [ max( self.state[0] -1, 0 ), self.state[1] ] )\n","    if action == 1:\n","      self.state = tuple( [ min(self.state[0] +1, 99 ), self.state[1] ] )\n","    if action == 2:\n","      self.state = tuple( [ self.state[0], max( self.state[1] -1, 0 ) ] )\n","    if action == 3:\n","      self.state = tuple( [ self.state[0], min( self.state[1] +1, 99) ] )\n","    if action == 4:\n","      self.state = self.state \n","\n","    # Cada segundo que pasa, la canción va acabando\n","    self.epoch = self.epoch + 1\n","\n","    # Estimando el premio\n","    volumen_total = np.array( self.state ).mean()\n","    if volumen_total > 60 and volumen_total < 65:\n","      reward = 10\n","      done = True\n","    else:\n","      reward = -1\n","\n","\n","    # Verificar si acabó la canción o se llego al valor esperado\n","    if self.duracion_cancion == self.epoch and not done: \n","      done = True\n","\n","    # Set placeholder for info\n","    info = {}\n","\n","    # Return step information\n","    return self.state, reward, done, self.epoch, info\n","\n","  def render(self):\n","    # Implement viz\n","    pass\n","\n","  def reset(self):\n","    # Reseteo de volumen\n","    self.state = tuple( [ random.randint(30,90), random.randint(30,90) ] )\n","    # Reseteo de canción\n","    self.duracion_cancion = 120\n","    # Reseteo de epoch\n","    self.epoch = 0\n","\n","    return self.state"]},{"cell_type":"markdown","metadata":{"id":"bXwGLejkeTVG"},"source":["### Creating the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":159,"status":"ok","timestamp":1679003035027,"user":{"displayName":"Christopher Orea","userId":"07171859992703395178"},"user_tz":360},"id":"-fD33wD4dYHs","outputId":"281d85b4-3176-4b3c-ad7a-3a8b61a8a915"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"]},{"data":{"text/plain":["(72, 77)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["env = ReguladorSonido()\n","env.reset()"]},{"cell_type":"markdown","metadata":{"id":"OZhpBryIXHaD"},"source":["## Q - Table"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1679002788265,"user":{"displayName":"Christopher Orea","userId":"07171859992703395178"},"user_tz":360},"id":"bceE2IX1XKRZ","outputId":"38e16b3d-cbc4-4d38-8730-8970b001c526"},"outputs":[{"data":{"text/plain":["Box(0.0, 100.0, (2,), float32)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["env.observation_space"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1679003136062,"user":{"displayName":"Christopher Orea","userId":"07171859992703395178"},"user_tz":360},"id":"klHh9oMQYc0L","outputId":"767d7b8e-d9aa-4984-9950-f7138c5dac46"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["env.action_space.sample()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDjZGe60ekDW"},"outputs":[],"source":["acciones = env.action_space.n\n","observaciones = env.observation_space.high[0].astype('int32')\n","q_table = np.zeros( [observaciones,observaciones, acciones] )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":170,"status":"ok","timestamp":1679002839741,"user":{"displayName":"Christopher Orea","userId":"07171859992703395178"},"user_tz":360},"id":"9Zg1ONjFhDn-","outputId":"e39eb6f0-c58c-47bf-9030-9ac68be4a4e7"},"outputs":[{"data":{"text/plain":["(100, 100, 5)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["q_table.shape"]},{"cell_type":"markdown","metadata":{"id":"LLtutcEdhtBi"},"source":["### Trainning the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"37XKl66hhbbk"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","# Hyperparameters\n","aprendizaje = 0.1\n","descuento = 0.9\n","epsilon = 0.1 # variabe control\n","\n","for i in range(1, 200001):\n","    estado = env.reset()\n","    done = False # bandera de finalización\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[estado]) # Exploit learned values\n","\n","        siguiente_estado, recompenza, done, _, info = env.step(action)\n","        \n","        valor_actual = q_table[estado][action]\n","        valor_futuro = np.max(q_table[siguiente_estado])\n","        \n","        nuevo_valor = valor_actual + ( aprendizaje * ( recompenza + ( descuento * valor_futuro ) - valor_actual ) )\n","        q_table[estado][action] = nuevo_valor\n","\n","        estado = siguiente_estado\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"markdown","metadata":{"id":"NGRLqbRno86-"},"source":["### Testing the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1679003898397,"user":{"displayName":"Christopher Orea","userId":"07171859992703395178"},"user_tz":360},"id":"LlVRixhDoysI","outputId":"221550bb-1b78-417b-a26b-539ea016198d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results after 100 episodes:\n","Average timesteps per episode: 26.01\n","Average rewards per episode: -16.22\n"]}],"source":["total_epochs, total_reward = 0, 0\n","episodes = 100\n","\n","env = ReguladorSonido()\n","\n","for _ in range(episodes):\n","    state = env.reset()\n","    env.render()\n","    epochs, reward = 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","      action = np.argmax(q_table[state])\n","      state, reward, done, _, info = env.step(action)\n","      epochs += 1\n","\n","      total_reward += reward\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average rewards per episode: {total_reward / episodes}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyH8n0cPaRMd"},"outputs":[],"source":["q_table"]},{"cell_type":"markdown","metadata":{"id":"gnfQORPbpdFT"},"source":["## Q-Learning Ejercicio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOQJe42ipdFT","outputId":"552ec7cf-3447-4973-fc3a-e23755b5a5f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gym[atari] in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (1.23.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (2.2.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (0.0.8)\n","Collecting ale-py~=0.8.0\n","  Downloading ale_py-0.8.0-cp310-cp310-win_amd64.whl (950 kB)\n","     -------------------------------------- 950.8/950.8 kB 1.1 MB/s eta 0:00:00\n","Requirement already satisfied: typing-extensions in c:\\users\\c28_0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ale-py~=0.8.0->gym[atari]) (4.4.0)\n","Collecting importlib-resources\n","  Downloading importlib_resources-5.10.2-py3-none-any.whl (34 kB)\n","Installing collected packages: importlib-resources, ale-py\n","Successfully installed ale-py-0.8.0 importlib-resources-5.10.2\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script ale-import-roms.exe is installed in 'C:\\Users\\c28_0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"]}],"source":["!pip install gym[atari]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QEuoU1uApdFT"},"outputs":[],"source":["import gym\n","import numpy as np\n","\n","env = gym.make(\"Taxi-v3\", render_mode='human')\n","\n","env.reset() # reset environment to a new, random state\n","env.render()\n","\n","\n","print(\"Acciones {}\".format(env.action_space))\n","print(\"Estados {}\".format(env.observation_space))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TiOT3D1HpdFU"},"outputs":[],"source":["env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wpr53CSUpdFU"},"outputs":[],"source":["env = gym.make(\"Taxi-v3\")\n","env.reset()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlQKCO5spdFV","outputId":"1baaad38-b08f-4eef-e6a4-23570a2241ab"},"outputs":[{"data":{"text/plain":["Discrete(500)"]},"execution_count":636,"metadata":{},"output_type":"execute_result"}],"source":["env.observation_space"]},{"cell_type":"markdown","metadata":{"id":"kp1e0EmpZoqy"},"source":["Create the Q table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4wdhC3xZqCP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVWGQshTpdFV","outputId":"40586b34-2166-421c-c87b-5744c17f3f40"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode: 100000\n","Training finished.\n","\n"]}],"source":["import random\n","from IPython.display import clear_output\n","\n","env = gym.make(\"Taxi-v3\")\n","\n","# Hyperparameters\n","aprendizaje = \n","descuento = \n","epsilon = \n","\n","for i in range(1, 100001):\n","    state = env.reset()[0]\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        siguiente_estado, recompenza, done, _, info = env.step(action) \n","        \n","        valor_actual = q_table[state, action]\n","        valor_futuro = np.max(q_table[siguiente_estado])\n","        \n","        nuevo_valor = valor_actual + ( aprendizaje * ( recompenza + ( descuento * valor_futuro ) - valor_actual ) )\n","        q_table[state, action] = nuevo_valor\n","\n","        state = siguiente_estado\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUswwLFypdFW","outputId":"6470bc2b-9995-498f-ade5-c7fa8715a699"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results after 100 episodes:\n","Average timesteps per episode: 12.84\n","Average penalties per episode: 0.0\n"]}],"source":["total_epochs, total_penalties = 0, 0\n","episodes = 100\n","\n","\n","for _ in range(episodes):\n","    env = gym.make(\"Taxi-v3\", render_mode='human')\n","    state = env.reset()[0]\n","    env.render()\n","    epochs, penalties, reward = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        action = np.argmax(q_table[state])\n","        state, reward, done, _, info = env.step(action)\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","    env.close()\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwN4wJo1pdFX"},"outputs":[],"source":["env.close()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.8 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"9c0c29ab910d078a8b86acec451c12e2964e85d2ce873541b902fdf373b0ea19"}}},"nbformat":4,"nbformat_minor":0}
